"""
ECGæ•°æ®åˆ†æè„šæœ¬

åŠŸèƒ½:
1. ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
2. åˆ†æECGä¿¡å·æ•°å€¼åˆ†å¸ƒ
3. æ£€æŸ¥æ•°æ®è´¨é‡
4. ç”Ÿæˆåˆ†ææŠ¥å‘Š

ç”¨æ³•:
    python data_analysis.py --data_dirs /path/to/dir1 /path/to/dir2 ...
"""

import os
import glob
import argparse
import json
from collections import defaultdict, Counter
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing

import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
plt.rcParams['font.sans-serif'] = ['PingFang SC', 'STKaiti', 'Arial Unicode MS']  # ä¼˜å…ˆä½¿ç”¨è‹¹æ–¹ï¼Œå…¶æ¬¡æ˜¯æ¥·ä½“
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
# è¯Šæ–­ç±»å‹æ˜ å°„
DIAGNOSIS_NAMES = {
    1: "çª¦æ€§å¿ƒå¾‹", 2: "å¿ƒç”µå›¾æœªè§å¼‚å¸¸", 3: "çª¦æ€§å¿ƒåŠ¨è¿‡é€Ÿ", 4: "çª¦æ€§å¿ƒåŠ¨è¿‡ç¼“",
    5: "çª¦æ€§åœæ", 6: "å¿ƒæˆ¿é¢¤åŠ¨", 7: "æˆ¿æ€§æ—©æ", 8: "å¶å‘æˆ¿æ€§æ—©æ",
    9: "é¢‘å‘æˆ¿æ€§æ—©æ", 10: "æˆ¿æ€§æ—©æäºŒè”å¾‹", 11: "æˆ¿æ€§æ—©æä¸‰è”å¾‹", 12: "æˆå¯¹æˆ¿æ€§æ—©æ",
    13: "çŸ­é˜µæˆ¿æ€§å¿ƒåŠ¨è¿‡é€Ÿ", 14: "å®¤æ€§æ—©æ", 15: "å¶å‘å®¤æ€§æ—©æ", 16: "é¢‘å‘å®¤æ€§æ—©æ",
    17: "å®¤æ€§æ—©æäºŒè”å¾‹", 18: "å®¤æ€§æ—©æä¸‰è”å¾‹", 19: "æˆå¯¹å®¤æ€§æ—©æ", 20: "çŸ­é˜µå®¤æ€§å¿ƒåŠ¨è¿‡é€Ÿ",
    21: "å®¤ä¸Šæ€§å¿ƒåŠ¨è¿‡é€Ÿ", 22: "ä¸€åº¦æˆ¿å®¤é˜»æ»", 23: "STæ®µæŠ¬é«˜", 24: "STæ®µå‹ä½",
    25: "QT/QTcé—´æœŸå»¶é•¿", 26: "RRé•¿é—´æ­‡", 27: "å¿ƒå®¤å†…å·®å¼‚ä¼ å¯¼", 28: "å¹²æ‰°æ³¢",
    29: "å¯¼è”è„±è½", 30: "å¿ƒæˆ¿æ‰‘åŠ¨", 31: "çŸ­PRé—´æœŸ", 32: "äºŒåº¦â…¡å‹æˆ¿å®¤é˜»æ»",
    33: "Pæ³¢å¢é«˜", 34: "Pæ³¢å¢å®½", 35: "ç–‘ä¼¼å·¦å³æ‰‹åæ¥å¿ƒç”µå›¾", 36: "Ræ³¢é«˜ç”µå‹",
    37: "å®¤å†…é˜»æ»", 38: "Tæ³¢æ”¹å˜", 39: "çŸ­QT/QTcé—´æœŸ", 40: "å¿ƒç”µå›¾æœªè§æ˜æ˜¾å¼‚å¸¸",
}


def parse_single_file(file_path):
    """è§£æå•ä¸ªæ–‡ä»¶ï¼Œè¿”å›æ ‡ç­¾å’Œä¿¡å·ç»Ÿè®¡ä¿¡æ¯"""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()

        # è§£ææ ‡ç­¾
        labels = []

        for i in range(2, len(lines)):  # ä»ç¬¬3è¡Œ(ç´¢å¼•2)å¼€å§‹
            try:
                val = int(lines[i].strip())
                if val == 250:  # é‡åˆ°é‡‡æ ·ç‡ï¼Œæ ‡ç­¾ç»“æŸ
                    break
                if 1 <= val <= 40:
                    labels.append(val)
            except:
                continue

        # if len(lines) > 2:
        #     try:
        #         diag1 = int(lines[2].strip())
        #         if 1 <= diag1 <= 40:
        #             labels.append(diag1)
        #     except:
        #         pass
        #
        # if len(lines) > 3:
        #     try:
        #         diag2 = int(lines[3].strip())
        #         if 1 <= diag2 <= 40:
        #             labels.append(diag2)
        #     except:
        #         pass

        # æ‰¾åˆ°32767(èµ·å§‹)å’Œ32763(ç»“æŸ)åˆ†éš”ç¬¦ä¹‹é—´çš„æ•°æ®
        data_start = 0
        data_end = len(lines)

        for i, line in enumerate(lines):
            val = line.strip()
            if val == '32767' and data_start == 0:
                data_start = i + 1  # 32767ä¹‹åå¼€å§‹
            elif val == '32763' and data_start > 0:
                data_end = i  # 32763ä¹‹å‰ç»“æŸ
                break

        # è§£æECGæ•°æ® (32767åˆ°32763ä¹‹é—´)
        # ç­–ç•¥: ä¿ç•™æ‰€æœ‰ä½ç½®ï¼Œå¼‚å¸¸å€¼ç”¨å‰ä¸€ä¸ªæœ‰æ•ˆå€¼å¡«å……
        ecg_values = []
        last_valid = 0.0
        invalid_count = 0
        total_points = 0

        for i in range(data_start, data_end):
            total_points += 1
            try:
                value = float(lines[i].strip())
                if -32768 <= value <= 32767:  # æœ‰æ•ˆå€¼
                    ecg_values.append(value)
                    last_valid = value
                else:
                    ecg_values.append(last_valid)
                    invalid_count += 1
            except:
                ecg_values.append(last_valid)
                invalid_count += 1

        if len(ecg_values) == 0:
            return None

        ecg_array = np.array(ecg_values, dtype=np.float32)

        # ä¿¡å·ç»Ÿè®¡
        signal_stats = {
            'length': len(ecg_array),
            'min': float(np.min(ecg_array)),
            'max': float(np.max(ecg_array)),
            'mean': float(np.mean(ecg_array)),
            'std': float(np.std(ecg_array)),
            'has_nan': bool(np.isnan(ecg_array).any()),
            'has_inf': bool(np.isinf(ecg_array).any()),
            'invalid_count': invalid_count,  # å¼‚å¸¸å€¼æ•°é‡
            'invalid_ratio': invalid_count / total_points if total_points > 0 else 0,  # å¼‚å¸¸å€¼æ¯”ä¾‹
        }

        return {
            'file': file_path,
            'labels': labels,
            'signal_stats': signal_stats
        }

    except Exception as e:
        return {'file': file_path, 'error': str(e)}


def analyze_data(data_dirs, num_workers=8, sample_ratio=1.0):
    """
    åˆ†ææ•°æ®åˆ†å¸ƒ

    Args:
        data_dirs: æ•°æ®ç›®å½•åˆ—è¡¨
        num_workers: å¹¶è¡Œå¤„ç†è¿›ç¨‹æ•°
        sample_ratio: é‡‡æ ·æ¯”ä¾‹ (1.0è¡¨ç¤ºå…¨éƒ¨åˆ†æ)
    """
    # æ”¶é›†æ‰€æœ‰æ–‡ä»¶
    all_files = []
    for data_dir in data_dirs:
        patterns = [
            os.path.join(data_dir, "*.txt"),
            os.path.join(data_dir, "**/*.txt"),
        ]
        for pattern in patterns:
            all_files.extend(glob.glob(pattern, recursive=True))

    all_files = list(set(all_files))
    print(f"æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶")

    # é‡‡æ ·
    if sample_ratio < 1.0:
        np.random.shuffle(all_files)
        sample_size = int(len(all_files) * sample_ratio)
        all_files = all_files[:sample_size]
        print(f"é‡‡æ · {sample_size} ä¸ªæ–‡ä»¶è¿›è¡Œåˆ†æ")

    # å¹¶è¡Œè§£æ
    results = []
    errors = []

    print(f"ä½¿ç”¨ {num_workers} ä¸ªè¿›ç¨‹å¹¶è¡Œåˆ†æ...")
    with ProcessPoolExecutor(max_workers=num_workers) as executor:
        futures = {executor.submit(parse_single_file, f): f for f in all_files}

        for future in tqdm(as_completed(futures), total=len(futures), desc="åˆ†æè¿›åº¦"):
            result = future.result()
            if result is None:
                continue
            if 'error' in result:
                errors.append(result)
            else:
                results.append(result)

    print(f"\næˆåŠŸè§£æ: {len(results)} ä¸ªæ–‡ä»¶")
    print(f"è§£æå¤±è´¥: {len(errors)} ä¸ªæ–‡ä»¶")

    return results, errors


def compute_statistics(results):
    """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""

    # ==================== ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡ ====================
    label_counts = Counter()
    label_cooccurrence = defaultdict(Counter)  # æ ‡ç­¾å…±ç°
    label_combinations = Counter()  # æ‰€æœ‰æ ‡ç­¾ç»„åˆ
    multi_label_count = 0
    single_label_count = 0
    zero_label_count = 0
    label_count_distribution = Counter()  # æ ‡ç­¾æ•°é‡åˆ†å¸ƒ

    for r in results:
        labels = r['labels']

        # ç»Ÿè®¡æ ‡ç­¾æ•°é‡åˆ†å¸ƒ
        label_count_distribution[len(labels)] += 1

        if len(labels) == 0:
            zero_label_count += 1
            continue

        if len(labels) > 1:
            multi_label_count += 1
        else:
            single_label_count += 1

        for label in labels:
            label_counts[label] += 1

        # æ ‡ç­¾ç»„åˆç»Ÿè®¡ - å°†æ ‡ç­¾æ’åºåä½œä¸ºkey
        combo_key = tuple(sorted(labels))
        label_combinations[combo_key] += 1

        # å…±ç°ç»Ÿè®¡ (ä»…åŒæ ‡ç­¾)
        if len(labels) == 2:
            label_cooccurrence[labels[0]][labels[1]] += 1
            label_cooccurrence[labels[1]][labels[0]] += 1

    # ==================== ä¿¡å·åˆ†å¸ƒç»Ÿè®¡ ====================
    lengths = []
    mins = []
    maxs = []
    means = []
    stds = []
    nan_count = 0
    inf_count = 0
    invalid_counts = []  # æ¯ä¸ªæ ·æœ¬çš„å¼‚å¸¸å€¼æ•°é‡
    invalid_ratios = []  # æ¯ä¸ªæ ·æœ¬çš„å¼‚å¸¸å€¼æ¯”ä¾‹

    for r in results:
        stats = r['signal_stats']
        lengths.append(stats['length'])
        mins.append(stats['min'])
        maxs.append(stats['max'])
        means.append(stats['mean'])
        stds.append(stats['std'])
        if stats['has_nan']:
            nan_count += 1
        if stats['has_inf']:
            inf_count += 1
        invalid_counts.append(stats.get('invalid_count', 0))
        invalid_ratios.append(stats.get('invalid_ratio', 0))

    signal_distribution = {
        'length': {
            'min': int(np.min(lengths)),
            'max': int(np.max(lengths)),
            'mean': float(np.mean(lengths)),
            'std': float(np.std(lengths)),
            'expected': 7500,
            'correct_ratio': sum(1 for l in lengths if l == 7500) / len(lengths)
        },
        'value_range': {
            'global_min': float(np.min(mins)),
            'global_max': float(np.max(maxs)),
            'mean_of_means': float(np.mean(means)),
            'mean_of_stds': float(np.mean(stds)),
            'percentile_1': float(np.percentile(mins, 1)),
            'percentile_99': float(np.percentile(maxs, 99)),
            'percentile_5': float(np.percentile(mins, 5)),
            'percentile_95': float(np.percentile(maxs, 95)),
        },
        'quality': {
            'nan_count': nan_count,
            'inf_count': inf_count,
            'nan_ratio': nan_count / len(results),
            'inf_ratio': inf_count / len(results),
            'invalid_total': int(np.sum(invalid_counts)),  # æ€»å¼‚å¸¸ç‚¹æ•°
            'invalid_samples': sum(1 for c in invalid_counts if c > 0),  # æœ‰å¼‚å¸¸å€¼çš„æ ·æœ¬æ•°
            'invalid_mean_ratio': float(np.mean(invalid_ratios)),  # å¹³å‡å¼‚å¸¸å€¼æ¯”ä¾‹
            'invalid_max_ratio': float(np.max(invalid_ratios)) if invalid_ratios else 0,  # æœ€å¤§å¼‚å¸¸å€¼æ¯”ä¾‹
        }
    }

    return {
        'total_samples': len(results),
        'single_label_count': single_label_count,
        'multi_label_count': multi_label_count,
        'zero_label_count': zero_label_count,
        'multi_label_ratio': multi_label_count / len(results) if results else 0,
        'label_counts': dict(label_counts),
        'label_cooccurrence': {k: dict(v) for k, v in label_cooccurrence.items()},
        'label_combinations': {str(k): v for k, v in label_combinations.items()},  # æ‰€æœ‰æ ‡ç­¾ç»„åˆ
        'label_count_distribution': dict(label_count_distribution),  # æ ‡ç­¾æ•°é‡åˆ†å¸ƒ
        'signal_distribution': signal_distribution,
    }


def print_report(stats):
    """æ‰“å°åˆ†ææŠ¥å‘Š"""

    print("\n" + "=" * 70)
    print("ECGæ•°æ®åˆ†ææŠ¥å‘Š")
    print("=" * 70)

    # åŸºæœ¬ä¿¡æ¯
    print(f"\nã€åŸºæœ¬ä¿¡æ¯ã€‘")
    print(f"  æ€»æ ·æœ¬æ•°: {stats['total_samples']:,}")
    print(f"  å•æ ‡ç­¾æ ·æœ¬: {stats['single_label_count']:,} ({stats['single_label_count']/stats['total_samples']*100:.1f}%)")
    print(f"  å¤šæ ‡ç­¾æ ·æœ¬: {stats['multi_label_count']:,} ({stats['multi_label_ratio']*100:.1f}%)")

    # ç±»åˆ«åˆ†å¸ƒ
    print(f"\nã€ç±»åˆ«åˆ†å¸ƒã€‘")
    print(f"  {'ID':<4} {'è¯Šæ–­åç§°':<20} {'æ ·æœ¬æ•°':>10} {'å æ¯”':>8} {'çŠ¶æ€'}")
    print("-" * 60)

    label_counts = stats['label_counts']
    total = stats['total_samples']
    sorted_labels = sorted(label_counts.items(), key=lambda x: x[1], reverse=True)

    for label_id, count in sorted_labels:
        name = DIAGNOSIS_NAMES.get(label_id, f"æœªçŸ¥{label_id}")
        ratio = count / total * 100

        # çŠ¶æ€åˆ¤æ–­
        if count < 100:
            status = "âš ï¸ æå°‘"
        elif count < 1000:
            status = "âš ï¸ è¾ƒå°‘"
        elif ratio > 30:
            status = "ğŸ“Š ä¸»å¯¼"
        else:
            status = "âœ“ æ­£å¸¸"

        print(f"  {label_id:<4} {name:<20} {count:>10,} {ratio:>7.2f}% {status}")

    # æœªå‡ºç°çš„ç±»åˆ«
    missing_labels = set(range(1, 41)) - set(label_counts.keys())
    if missing_labels:
        print(f"\n  âš ï¸ æœªå‡ºç°çš„ç±»åˆ«: {sorted(missing_labels)}")

    # ç±»åˆ«ä¸å¹³è¡¡åˆ†æ
    if label_counts:
        max_count = max(label_counts.values())
        min_count = min(label_counts.values())
        imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')
        print(f"\n  ç±»åˆ«ä¸å¹³è¡¡æ¯”: {imbalance_ratio:.1f}:1 (æœ€å¤š/æœ€å°‘)")

    # æ ‡ç­¾æ•°é‡åˆ†å¸ƒ
    print(f"\nã€æ¯æ ·æœ¬æ ‡ç­¾æ•°é‡åˆ†å¸ƒã€‘")
    label_count_dist = stats.get('label_count_distribution', {})
    for num_labels in sorted(label_count_dist.keys()):
        count = label_count_dist[num_labels]
        ratio = count / stats['total_samples'] * 100
        print(f"  {num_labels}ä¸ªæ ‡ç­¾: {count:,} æ ·æœ¬ ({ratio:.2f}%)")

    if stats.get('zero_label_count', 0) > 0:
        print(f"  âš ï¸ æ— æ ‡ç­¾æ ·æœ¬: {stats['zero_label_count']:,}")

    # æ‰€æœ‰æ ‡ç­¾ç»„åˆ (å®Œæ•´ç»Ÿè®¡)
    print(f"\nã€æ‰€æœ‰æ ‡ç­¾ç»„åˆ (å®Œæ•´åˆ—è¡¨)ã€‘")
    label_combinations = stats.get('label_combinations', {})

    # æŒ‰å‡ºç°æ¬¡æ•°æ’åº
    sorted_combos = sorted(label_combinations.items(), key=lambda x: x[1], reverse=True)

    # åˆ†ç»„æ˜¾ç¤ºï¼šå•æ ‡ç­¾ã€åŒæ ‡ç­¾ã€å¤šæ ‡ç­¾
    single_combos = [(k, v) for k, v in sorted_combos if len(eval(k)) == 1]
    double_combos = [(k, v) for k, v in sorted_combos if len(eval(k)) == 2]
    multi_combos = [(k, v) for k, v in sorted_combos if len(eval(k)) > 2]

    print(f"\n  --- å•æ ‡ç­¾ç»„åˆ ({len(single_combos)}ç§) ---")
    for combo_str, count in single_combos:
        combo = eval(combo_str)
        names = [DIAGNOSIS_NAMES.get(l, f"ç±»åˆ«{l}") for l in combo]
        print(f"  [{combo[0]:2d}] {names[0]}: {count:,}")

    print(f"\n  --- åŒæ ‡ç­¾ç»„åˆ ({len(double_combos)}ç§) ---")
    for combo_str, count in double_combos:
        combo = eval(combo_str)
        names = [DIAGNOSIS_NAMES.get(l, f"ç±»åˆ«{l}")[:12] for l in combo]
        print(f"  [{combo[0]:2d},{combo[1]:2d}] {names[0]} + {names[1]}: {count:,}")

    if multi_combos:
        print(f"\n  --- ä¸‰æ ‡ç­¾åŠä»¥ä¸Šç»„åˆ ({len(multi_combos)}ç§) ---")
        for combo_str, count in multi_combos:
            combo = eval(combo_str)
            label_ids = ','.join(str(l) for l in combo)
            names = ' + '.join(DIAGNOSIS_NAMES.get(l, f"ç±»åˆ«{l}")[:8] for l in combo)
            print(f"  [{label_ids}] {names}: {count:,}")

    print(f"\n  å…± {len(sorted_combos)} ç§ä¸åŒçš„æ ‡ç­¾ç»„åˆ")

    # ä¿¡å·åˆ†å¸ƒ
    print(f"\nã€ECGä¿¡å·åˆ†å¸ƒã€‘")
    sig = stats['signal_distribution']

    print(f"  åºåˆ—é•¿åº¦:")
    print(f"    é¢„æœŸ: {sig['length']['expected']}")
    print(f"    å®é™…: {sig['length']['min']} ~ {sig['length']['max']}")
    print(f"    æ­£ç¡®ç‡: {sig['length']['correct_ratio']*100:.1f}%")

    print(f"\n  ECGä¿¡å·æ•°å€¼èŒƒå›´ (åŸºäºåˆ†ä½æ•°ï¼Œæ’é™¤å¼‚å¸¸å€¼):")
    print(f"    1%-99%åˆ†ä½: [{sig['value_range']['percentile_1']:.2f}, {sig['value_range']['percentile_99']:.2f}]")
    print(f"    5%-95%åˆ†ä½: [{sig['value_range']['percentile_5']:.2f}, {sig['value_range']['percentile_95']:.2f}]")
    print(f"    å„æ ·æœ¬å‡å€¼çš„å‡å€¼: {sig['value_range']['mean_of_means']:.4f}")
    print(f"    å„æ ·æœ¬æ ‡å‡†å·®çš„å‡å€¼: {sig['value_range']['mean_of_stds']:.4f}")
    print(f"    (å…¨å±€æå€¼: [{sig['value_range']['global_min']:.2f}, {sig['value_range']['global_max']:.2f}] - å¯èƒ½å«è¾¹ç•Œå€¼)")

    print(f"\n  æ•°æ®è´¨é‡:")
    print(f"    å«NaN: {sig['quality']['nan_count']} ({sig['quality']['nan_ratio']*100:.2f}%)")
    print(f"    å«Inf: {sig['quality']['inf_count']} ({sig['quality']['inf_ratio']*100:.2f}%)")
    print(f"    å¼‚å¸¸å€¼ç‚¹ä½: {sig['quality']['invalid_total']:,} (å·²ç”¨å‰å€¼å¡«å……)")
    print(f"    å«å¼‚å¸¸å€¼çš„æ ·æœ¬: {sig['quality']['invalid_samples']} ({sig['quality']['invalid_samples']/stats['total_samples']*100:.2f}%)")
    print(f"    å¹³å‡å¼‚å¸¸å€¼æ¯”ä¾‹: {sig['quality']['invalid_mean_ratio']*100:.4f}%")
    if sig['quality']['invalid_max_ratio'] > 0.01:
        print(f"    âš ï¸ æœ€å¤§å¼‚å¸¸å€¼æ¯”ä¾‹: {sig['quality']['invalid_max_ratio']*100:.2f}% (>1%éœ€å…³æ³¨)")

    # å½’ä¸€åŒ–å»ºè®®
    print(f"\nã€å½’ä¸€åŒ–å»ºè®®ã€‘")
    if abs(sig['value_range']['mean_of_means']) < 0.1 and 0.5 < sig['value_range']['mean_of_stds'] < 2:
        print("  âœ“ æ•°æ®å·²æ¥è¿‘æ ‡å‡†åŒ–ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨æˆ–åšè½»å¾®è°ƒæ•´")
    else:
        print("  âš ï¸ å»ºè®®è¿›è¡ŒZ-scoreæ ‡å‡†åŒ–:")
        print(f"     normalized = (x - mean) / std")
        print(f"     æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹æ ‡å‡†åŒ–ï¼Œä½¿å‡å€¼â‰ˆ0ï¼Œæ ‡å‡†å·®â‰ˆ1")

    print("\n" + "=" * 70)


def plot_distribution(stats, save_path='data_distribution.png'):
    """ç»˜åˆ¶åˆ†å¸ƒå›¾"""

    fig = plt.figure(figsize=(40, 30))

    # åˆ›å»ºå­å›¾å¸ƒå±€: 3è¡Œ2åˆ—
    gs = fig.add_gridspec(3, 2, height_ratios=[1.5, 1, 1], hspace=0.3, wspace=0.3)

    label_counts = stats['label_counts']
    sorted_items = sorted(label_counts.items(), key=lambda x: x[1], reverse=True)

    # ==================== 1. å…¨éƒ¨40ç±»åˆ«æ ·æœ¬æ•°åˆ†å¸ƒ (æ¨ªå‘æ¡å½¢å›¾) ====================
    # ax1 = fig.add_subplot(gs[0, :])  # å æ®ç¬¬ä¸€è¡Œä¸¤åˆ—
    #
    # # æŒ‰ç±»åˆ«IDæ’åºæ˜¾ç¤ºæ‰€æœ‰40ä¸ªç±»åˆ«
    all_class_ids = list(range(1, 41))
    all_counts = [label_counts.get(i, 0) for i in all_class_ids]
    all_names = [f"{i}.{DIAGNOSIS_NAMES.get(i, f'ç±»åˆ«{i}')[:6]}" for i in all_class_ids]
    #
    # # æ ¹æ®æ ·æœ¬æ•°é‡è®¾ç½®é¢œè‰²
    # colors = []
    # for c in all_counts:
    #     if c == 0:
    #         colors.append('#d62728')  # çº¢è‰² - æ— æ ·æœ¬
    #     elif c < 100:
    #         colors.append('#ff7f0e')  # æ©™è‰² - æå°‘ (<100)
    #     elif c < 1000:
    #         colors.append('#ffbb78')  # æµ…æ©™ - è¾ƒå°‘ (<1000)
    #     elif c < 5000:
    #         colors.append('#98df8a')  # æµ…ç»¿ - ä¸­ç­‰
    #     else:
    #         colors.append('#2ca02c')  # æ·±ç»¿ - å……è¶³ (>5000)
    #
    # y_pos = np.arange(len(all_class_ids))
    # bars = ax1.barh(y_pos, all_counts, color=colors, edgecolor='white', height=0.8)
    #
    # ax1.set_yticks(y_pos)
    # ax1.set_yticklabels(all_names, fontsize=8)
    # ax1.set_xlabel('æ ·æœ¬æ•°', fontsize=10)
    # ax1.set_title('å…¨éƒ¨40ç±»åˆ«æ ·æœ¬æ•°åˆ†å¸ƒ (æŒ‰ç±»åˆ«IDæ’åº)', fontsize=12, fontweight='bold')
    # ax1.invert_yaxis()
    #
    # # åœ¨æ¡å½¢ä¸Šæ˜¾ç¤ºæ•°å€¼
    # for bar, count in zip(bars, all_counts):
    #     if count > 0:
    #         ax1.text(bar.get_width() + max(all_counts) * 0.01, bar.get_y() + bar.get_height()/2,
    #                 f'{count:,}', va='center', fontsize=7)
    #     else:
    #         ax1.text(max(all_counts) * 0.01, bar.get_y() + bar.get_height()/2,
    #                 'æ— æ ·æœ¬', va='center', fontsize=7, color='red')
    #
    # # æ·»åŠ å›¾ä¾‹
    # from matplotlib.patches import Patch
    # legend_elements = [
    #     Patch(facecolor='#2ca02c', label='å……è¶³ (>5000)'),
    #     Patch(facecolor='#98df8a', label='ä¸­ç­‰ (1000-5000)'),
    #     Patch(facecolor='#ffbb78', label='è¾ƒå°‘ (100-1000)'),
    #     Patch(facecolor='#ff7f0e', label='æå°‘ (<100)'),
    #     Patch(facecolor='#d62728', label='æ— æ ·æœ¬ (0)'),
    # ]
    # ax1.legend(handles=legend_elements, loc='lower right', fontsize=8)

    # ==================== 1. Top 10 ç±»åˆ«æ ·æœ¬æ•°åˆ†å¸ƒ (æŒ‰æ ·æœ¬æ•°æ’åº) ====================
    ax1 = fig.add_subplot(gs[0, :])  # å æ®ç¬¬ä¸€è¡Œä¸¤åˆ—

    # 1. æ•°æ®å‡†å¤‡ï¼šæŒ‰æ ·æœ¬æ•°é™åºæ’åºï¼Œå¹¶å–å‰10
    # label_counts æ˜¯ {id: count}
    sorted_items = sorted(label_counts.items(), key=lambda x: x[1], reverse=True)[:20]

    # è§£åŒ…æ•°æ®
    top_ids = [item[0] for item in sorted_items]
    top_counts = [item[1] for item in sorted_items]
    # ç”Ÿæˆæ˜¾ç¤ºåç§° "ID.åç§°"
    top_names = [f"{DIAGNOSIS_NAMES.get(int(i), f'ç±»åˆ«{i}')}" for i in top_ids]

    # 2. æ ¹æ®æ ·æœ¬æ•°é‡è®¾ç½®é¢œè‰²
    colors = []
    for c in top_counts:
        if c == 0:
            colors.append('#d62728')  # çº¢è‰²
        elif c < 100:
            colors.append('#ff7f0e')  # æ©™è‰²
        elif c < 1000:
            colors.append('#ffbb78')  # æµ…æ©™
        elif c < 5000:
            colors.append('#98df8a')  # æµ…ç»¿
        else:
            colors.append('#2ca02c')  # æ·±ç»¿

    # 3. ç»˜å›¾
    y_pos = np.arange(len(top_ids))
    # æ³¨æ„ï¼šbarhé»˜è®¤ä»ä¸‹å¾€ä¸Šç”»(0åœ¨ä¸‹)ï¼Œä¸ºäº†è®©Top1åœ¨æœ€ä¸Šé¢ï¼Œæˆ‘ä»¬åé¢ä¼šinvert_yaxis
    bars = ax1.barh(y_pos, top_counts, color=colors, edgecolor='white', height=0.7)

    # 4. è®¾ç½®è½´å’Œæ ‡ç­¾
    ax1.set_yticks(y_pos)
    ax1.set_yticklabels(top_names, fontsize=10)  # åªæœ‰10ä¸ªï¼Œå­—ä½“å¯ä»¥ç¨å¤§
    ax1.set_xlabel('æ ·æœ¬æ•°', fontsize=10)
    ax1.set_title(f'æ ·æœ¬æ•° Top {len(top_ids)} ç±»åˆ«åˆ†å¸ƒ', fontsize=12, fontweight='bold')

    # åè½¬Yè½´ï¼Œä½¿ç¬¬ä¸€ä¸ªå…ƒç´ ï¼ˆTop 1ï¼‰æ˜¾ç¤ºåœ¨æœ€ä¸Šæ–¹
    ax1.invert_yaxis()

    # 5. åœ¨æ¡å½¢ä¸Šæ˜¾ç¤ºæ•°å€¼ (å¢åŠ å æ¯”æ˜¾ç¤º)
    total_samples = stats['total_samples']
    max_val = max(top_counts) if top_counts else 0

    for bar, count in zip(bars, top_counts):
        # è®¡ç®—æ˜¾ç¤ºä½ç½®
        width = bar.get_width()
        # æ–‡æœ¬å†…å®¹ï¼šæ•°é‡ + (å æ¯”)
        ratio = (count / total_samples * 100) if total_samples > 0 else 0
        label_text = f'{count:,} ({ratio:.1f}%)'

        ax1.text(width + max_val * 0.01, bar.get_y() + bar.get_height() / 2,
                 label_text, va='center', fontsize=9)

    # 6. æ·»åŠ å›¾ä¾‹ (ä¿æŒåŸæœ‰é€»è¾‘ä»¥è§£é‡Šé¢œè‰²å«ä¹‰)
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor='#2ca02c', label='å……è¶³ (>5000)'),
        Patch(facecolor='#98df8a', label='ä¸­ç­‰ (1000-5000)'),
        Patch(facecolor='#ffbb78', label='è¾ƒå°‘ (100-1000)'),
        Patch(facecolor='#ff7f0e', label='æå°‘ (<100)'),
    ]
    ax1.legend(handles=legend_elements, loc='lower right', fontsize=8)

    # ==================== 2. ç±»åˆ«æ ·æœ¬æ•°å¯¹æ•°åˆ†å¸ƒ ====================
    ax2 = fig.add_subplot(gs[1, 0])

    # æŒ‰æ ·æœ¬æ•°æ’åº
    sorted_counts = sorted(all_counts, reverse=True)
    x_pos = np.arange(len(sorted_counts))

    ax2.bar(x_pos, sorted_counts, color='steelblue', edgecolor='white')
    ax2.set_yscale('log')  # å¯¹æ•°åˆ»åº¦æ›´å®¹æ˜“çœ‹å‡ºå·®å¼‚
    ax2.set_xlabel('ç±»åˆ«æ’å (æŒ‰æ ·æœ¬æ•°é™åº)', fontsize=10)
    ax2.set_ylabel('æ ·æœ¬æ•° (å¯¹æ•°åˆ»åº¦)', fontsize=10)
    ax2.set_title('ç±»åˆ«æ ·æœ¬æ•°åˆ†å¸ƒ (å¯¹æ•°åˆ»åº¦)', fontsize=11, fontweight='bold')

    # æ·»åŠ å‚è€ƒçº¿
    ax2.axhline(y=1000, color='orange', linestyle='--', alpha=0.7, label='1000æ ·æœ¬çº¿')
    ax2.axhline(y=100, color='red', linestyle='--', alpha=0.7, label='100æ ·æœ¬çº¿')
    ax2.legend(fontsize=8)

    # æ ‡æ³¨ç»Ÿè®¡ä¿¡æ¯
    non_zero_counts = [c for c in all_counts if c > 0]
    if non_zero_counts:
        median_val = np.median(non_zero_counts)
        ax2.text(0.95, 0.95, f'æœ‰æ ·æœ¬ç±»åˆ«: {len(non_zero_counts)}/40\n'
                            f'ä¸­ä½æ•°: {median_val:,.0f}\n'
                            f'æœ€å¤§: {max(non_zero_counts):,}\n'
                            f'æœ€å°(é0): {min(non_zero_counts):,}',
                transform=ax2.transAxes, fontsize=9, va='top', ha='right',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    # ==================== 3. å•æ ‡ç­¾ vs å¤šæ ‡ç­¾ ====================
    ax3 = fig.add_subplot(gs[1, 1])

    single = stats['single_label_count']
    multi = stats['multi_label_count']
    zero = stats.get('zero_label_count', 0)

    if zero > 0:
        categories = ['å•æ ‡ç­¾', 'å¤šæ ‡ç­¾', 'æ— æ ‡ç­¾']
        values = [single, multi, zero]
        bar_colors = ['#2ca02c', '#1f77b4', '#d62728']
    else:
        categories = ['å•æ ‡ç­¾', 'å¤šæ ‡ç­¾']
        values = [single, multi]
        bar_colors = ['#2ca02c', '#1f77b4']

    bars = ax3.bar(categories, values, color=bar_colors, edgecolor='white')
    ax3.set_ylabel('æ ·æœ¬æ•°', fontsize=10)
    ax3.set_title(f'æ ‡ç­¾æ•°é‡åˆ†å¸ƒ (å¤šæ ‡ç­¾å æ¯”: {stats["multi_label_ratio"]*100:.1f}%)',
                  fontsize=11, fontweight='bold')

    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼å’Œç™¾åˆ†æ¯”
    total = sum(values)
    for bar, val in zip(bars, values):
        pct = val / total * 100 if total > 0 else 0
        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + total * 0.01,
                f'{val:,},({pct:.1f}%)', ha='center', va='bottom', fontsize=9)

    # ==================== 4. æ¯æ ·æœ¬æ ‡ç­¾æ•°é‡åˆ†å¸ƒ ====================
    ax4 = fig.add_subplot(gs[2, 0])

    label_count_dist = stats.get('label_count_distribution', {})
    if label_count_dist:
        x_labels = sorted(label_count_dist.keys())
        y_values = [label_count_dist[k] for k in x_labels]

        bars = ax4.bar([str(x) for x in x_labels], y_values, color='#9467bd', edgecolor='white')
        ax4.set_xlabel('æ¯æ ·æœ¬çš„æ ‡ç­¾æ•°é‡', fontsize=10)
        ax4.set_ylabel('æ ·æœ¬æ•°', fontsize=10)
        ax4.set_title('æ¯æ ·æœ¬æ ‡ç­¾æ•°é‡åˆ†å¸ƒ', fontsize=11, fontweight='bold')

        # æ˜¾ç¤ºæ•°å€¼
        for bar, val in zip(bars, y_values):
            if val > 0:
                ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(y_values) * 0.01,
                        f'{val:,}', ha='center', va='bottom', fontsize=8)

    # ==================== 5. Top 15 ç±»åˆ«å æ¯”é¥¼å›¾ ====================
    ax5 = fig.add_subplot(gs[2, 1])

    top_n = 10
    top_items = sorted_items[:top_n]
    others_count = sum(v for k, v in sorted_items[top_n:])

    pie_labels = [DIAGNOSIS_NAMES.get(int(k), f"ç±»åˆ«{k}") for k, v in top_items]
    pie_values = [v for k, v in top_items]

    if others_count > 0:
        pie_labels.append(f'å…¶ä»–{len(sorted_items)-top_n}ç±»')
        pie_values.append(others_count)

    # ä½¿ç”¨æ›´å¥½çœ‹çš„é¢œè‰²
    cmap = plt.cm.Set3
    pie_colors = [cmap(i / len(pie_values)) for i in range(len(pie_values))]

    wedges, texts, autotexts = ax5.pie(pie_values, labels=pie_labels, autopct='%1.1f%%',
                                        colors=pie_colors, pctdistance=0.75,
                                        wedgeprops=dict(width=0.5, edgecolor='white'))

    ax5.set_title(f'Top {top_n} ç±»åˆ«å æ¯”', fontsize=11, fontweight='bold')

    # è°ƒæ•´å­—ä½“å¤§å°
    for text in texts:
        text.set_fontsize(15)
    for autotext in autotexts:
        autotext.set_fontsize(8)

    plt.savefig(save_path, dpi=200, bbox_inches='tight')
    print(f"\nåˆ†å¸ƒå›¾å·²ä¿å­˜: {save_path}")





def main():
    parser = argparse.ArgumentParser(description='ECGæ•°æ®åˆ†æ')
    parser.add_argument('--data_dirs', nargs='+', required=True, help='æ•°æ®ç›®å½•åˆ—è¡¨')
    parser.add_argument('--output', type=str, default='data_analysis_report.json', help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶')
    parser.add_argument('--workers', type=int, default=32, help='å¹¶è¡Œè¿›ç¨‹æ•°')
    parser.add_argument('--sample_ratio', type=float, default=1.0, help='é‡‡æ ·æ¯”ä¾‹')
    parser.add_argument('--plot', action='store_true', help='ç”Ÿæˆåˆ†å¸ƒå›¾',default=True)
    args = parser.parse_args()

    # åˆ†ææ•°æ®
    results, errors = analyze_data(args.data_dirs, args.workers, args.sample_ratio)

    # è®¡ç®—ç»Ÿè®¡
    stats = compute_statistics(results)

    # æ‰“å°æŠ¥å‘Š
    print_report(stats)

    # ä¿å­˜æŠ¥å‘Š
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    print(f"\næŠ¥å‘Šå·²ä¿å­˜: {args.output}")

    # ç»˜å›¾
    if args.plot:
        plot_distribution(stats)

    # ä¿å­˜é”™è¯¯æ–‡ä»¶åˆ—è¡¨
    if errors:
        error_file = args.output.replace('.json', '_errors.json')
        with open(error_file, 'w', encoding='utf-8') as f:
            json.dump(errors, f, ensure_ascii=False, indent=2)
        print(f"é”™è¯¯æ–‡ä»¶åˆ—è¡¨: {error_file}")


if __name__ == "__main__":
    main()
    # with open('data_analysis_report.json', 'r', encoding='utf-8') as f:
    #     stats = json.load(f)
    # plot_distribution(stats)
