"""
æ•°æ®å»é‡è„šæœ¬ - ç¨³å®šå“ˆå¸Œç‰ˆæœ¬

ä¿®å¤å“ˆå¸Œä¸ä¸€è‡´é—®é¢˜ï¼Œä½¿ç”¨MD5ç¡®ä¿ç¨³å®šæ€§
"""

import os
import glob
import json
import argparse
import hashlib
from collections import defaultdict
from concurrent.futures import ProcessPoolExecutor, as_completed
import numpy as np
from tqdm import tqdm


def parse_ecg_signal_stable(file_path):
    """
    è§£æECGæ–‡ä»¶ï¼Œè¿”å›ä¿¡å·æ•°æ®ç”¨äºå»é‡æ£€æŸ¥

    å…³é”®æ”¹è¿›ï¼šä½¿ç”¨ç¨³å®šçš„MD5å“ˆå¸Œç®—æ³•
    """
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()

        # æ‰¾åˆ°æ•°æ®åŒºé—´
        data_start = 0
        data_end = len(lines)

        for i, line in enumerate(lines):
            val = line.strip()
            if val == '32767' and data_start == 0:
                data_start = i + 1
            elif val == '32763' and data_start > 0:
                data_end = i
                break

        # è§£ææ•°æ®
        ecg_values = []
        last_valid = 0.0

        for i in range(data_start, data_end):
            try:
                value = float(lines[i].strip())
                if -32768 <= value <= 32767:
                    ecg_values.append(value)
                    last_valid = value
                else:
                    ecg_values.append(last_valid)
            except:
                ecg_values.append(last_valid)

        if len(ecg_values) == 0:
            return None

        # å…³é”®æ”¹è¿›ï¼šä½¿ç”¨MD5å“ˆå¸Œï¼Œç¡®ä¿ç¨³å®šæ€§
        ecg_array = np.array(ecg_values, dtype=np.float32)

        # ä½¿ç”¨MD5è€Œä¸æ˜¯Pythonå†…ç½®çš„hash()
        signal_hash = hashlib.md5(ecg_array.tobytes()).hexdigest()

        file_size = os.path.getsize(file_path)

        return (file_path, signal_hash, file_size)

    except Exception as e:
        print(f"è§£æå¤±è´¥: {file_path}, é”™è¯¯: {e}")
        return None


def deduplicate_files_stable(file_list, num_workers=32):
    """
    å»é‡æ–‡ä»¶åˆ—è¡¨ - ç¨³å®šç‰ˆæœ¬
    """
    print(f"\nå¤„ç† {len(file_list)} ä¸ªæ–‡ä»¶...")

    # å¹¶è¡Œè§£ææ–‡ä»¶
    results = []
    failed_count = 0
    with ProcessPoolExecutor(max_workers=num_workers) as executor:
        futures = {executor.submit(parse_ecg_signal_stable, f): f for f in file_list}

        for future in tqdm(as_completed(futures), total=len(futures), desc="è§£ææ–‡ä»¶"):
            result = future.result()
            if result:
                results.append(result)
            else:
                failed_count += 1

    print(f"æˆåŠŸè§£æ: {len(results)} ä¸ªæ–‡ä»¶")
    if failed_count > 0:
        print(f"å¤±è´¥æ–‡ä»¶: {failed_count} ä¸ª")

    # æŒ‰å“ˆå¸Œåˆ†ç»„
    hash_groups = defaultdict(list)
    for file_path, signal_hash, file_size in results:
        hash_groups[signal_hash].append((file_path, file_size))

    # ç»Ÿè®¡
    unique_count = len(hash_groups)
    duplicate_groups = {h: files for h, files in hash_groups.items() if len(files) > 1}
    total_duplicates = sum(len(files) - 1 for files in duplicate_groups.values())

    print(f"\nå»é‡ç»“æœ:")
    print(f"  è§£ææˆåŠŸ: {len(results):,} ä¸ªæ–‡ä»¶")
    print(f"  å”¯ä¸€ä¿¡å·æ•°: {unique_count:,}")
    print(f"  é‡å¤ç»„æ•°: {len(duplicate_groups):,}")
    print(f"  é‡å¤æ–‡ä»¶æ•°: {total_duplicates:,}")
    if len(results) > 0:
        print(f"  å»é‡ç‡: {total_duplicates/len(results)*100:.2f}%")

    # ä»æ¯ç»„ä¸­é€‰æ‹©ä¸€ä¸ªæ–‡ä»¶ï¼ˆé€‰æ‹©æ–‡ä»¶åæœ€çŸ­çš„ï¼Œé€šå¸¸æ˜¯åŸå§‹æ–‡ä»¶ï¼‰
    unique_files = []
    for signal_hash, files in hash_groups.items():
        # æŒ‰æ–‡ä»¶åé•¿åº¦æ’åºï¼Œé€‰æ‹©æœ€çŸ­çš„
        files.sort(key=lambda x: (len(x[0]), x[0]))
        unique_files.append(files[0][0])

    return unique_files, duplicate_groups

def parse_ecg_signal_with_labels(file_path, num_classes=40):
    """
    è§£æECGæ–‡ä»¶ï¼Œè¿”å›ä¿¡å·æ•°æ®å’Œæ ‡ç­¾ç”¨äºå»é‡æ£€æŸ¥

    å…³é”®æ”¹è¿›ï¼š
    1. ä½¿ç”¨ç¨³å®šçš„MD5å“ˆå¸Œç®—æ³•
    2. åŒæ—¶è§£æECGä¿¡å·å’Œæ ‡ç­¾
    3. ç»„åˆä¿¡å·+æ ‡ç­¾ä½œä¸ºå”¯ä¸€æ ‡è¯†
    """
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()

        # ===== è§£ææ ‡ç­¾ï¼ˆä¸preprocess.pyç›¸åŒé€»è¾‘ï¼‰ =====
        labels = []

        for i in range(2, len(lines)):  # ä»ç¬¬3è¡Œ(ç´¢å¼•2)å¼€å§‹
            try:
                val = int(lines[i].strip())
                if val == 250:  # é‡åˆ°é‡‡æ ·ç‡ï¼Œæ ‡ç­¾ç»“æŸ
                    break
                if 1 <= val <= num_classes:
                    labels.append(val)
            except:
                continue

        # ===== è§£æECGä¿¡å· =====
        # æ‰¾åˆ°æ•°æ®åŒºé—´
        data_start = 0
        data_end = len(lines)

        for i, line in enumerate(lines):
            val = line.strip()
            if val == '32767' and data_start == 0:
                data_start = i + 1
            elif val == '32763' and data_start > 0:
                data_end = i
                break

        # è§£ææ•°æ®
        ecg_values = []
        last_valid = 0.0

        for i in range(data_start, data_end):
            try:
                value = float(lines[i].strip())
                if -32768 <= value <= 32767:
                    ecg_values.append(value)
                    last_valid = value
                else:
                    ecg_values.append(last_valid)
            except:
                ecg_values.append(last_valid)

        if len(ecg_values) == 0:
            return None

        # ===== åˆ›å»ºç»„åˆå“ˆå¸Œï¼ˆä¿¡å·+æ ‡ç­¾ï¼‰ =====
        ecg_array = np.array(ecg_values, dtype=np.float32)

        # å°†æ ‡ç­¾è½¬æ¢ä¸ºæ•°ç»„
        labels_array = np.array(labels, dtype=np.int32)

        # ç»„åˆä¿¡å·å’Œæ ‡ç­¾æ•°æ®
        combined_data = np.concatenate([ecg_array, labels_array.astype(np.float32)])

        # ä½¿ç”¨MD5è®¡ç®—ç»„åˆå“ˆå¸Œ
        content_hash = hashlib.md5(combined_data.tobytes()).hexdigest()

        file_size = os.path.getsize(file_path)

        return (file_path, content_hash, file_size, labels)

    except Exception as e:
        print(f"è§£æå¤±è´¥: {file_path}, é”™è¯¯: {e}")
        return None


def deduplicate_files_with_labels(file_list, num_classes=40, num_workers=32):
    """
    å»é‡æ–‡ä»¶åˆ—è¡¨ - ECG+æ ‡ç­¾ç»„åˆç‰ˆæœ¬

    å…³é”®æ”¹è¿›ï¼š
    1. åŸºäºECGä¿¡å·+æ ‡ç­¾ç»„åˆè¿›è¡Œå»é‡
    2. åªæœ‰ä¿¡å·å’Œæ ‡ç­¾éƒ½ç›¸åŒæ‰è®¤ä¸ºæ˜¯é‡å¤
    """
    print(f"\nå¤„ç† {len(file_list)} ä¸ªæ–‡ä»¶...")
    print(f"  ç±»åˆ«æ•°: {num_classes}")

    # å¹¶è¡Œè§£ææ–‡ä»¶ï¼ˆåŒ…å«æ ‡ç­¾ï¼‰
    results = []
    failed_count = 0
    with ProcessPoolExecutor(max_workers=num_workers) as executor:
        futures = {
            executor.submit(parse_ecg_signal_with_labels, f, num_classes): f
            for f in file_list
        }

        for future in tqdm(as_completed(futures), total=len(futures), desc="è§£ææ–‡ä»¶ï¼ˆå«æ ‡ç­¾ï¼‰"):
            result = future.result()
            if result:
                results.append(result)
            else:
                failed_count += 1

    print(f"æˆåŠŸè§£æ: {len(results)} ä¸ªæ–‡ä»¶")
    if failed_count > 0:
        print(f"å¤±è´¥æ–‡ä»¶: {failed_count} ä¸ª")

    # æŒ‰ç»„åˆå“ˆå¸Œåˆ†ç»„
    hash_groups = defaultdict(list)
    labels_info = {}  # å­˜å‚¨æ¯ä¸ªå“ˆå¸Œå¯¹åº”çš„æ ‡ç­¾ä¿¡æ¯
    for file_path, content_hash, file_size, labels in results:
        hash_groups[content_hash].append((file_path, file_size))
        if content_hash not in labels_info:
            labels_info[content_hash] = labels

    # ç»Ÿè®¡
    unique_count = len(hash_groups)
    duplicate_groups = {h: files for h, files in hash_groups.items() if len(files) > 1}
    total_duplicates = sum(len(files) - 1 for files in duplicate_groups.values())

    print(f"\nå»é‡ç»“æœ:")
    print(f"  è§£ææˆåŠŸ: {len(results):,} ä¸ªæ–‡ä»¶")
    print(f"  å”¯ä¸€ç»„åˆæ•°: {unique_count:,}")
    print(f"  é‡å¤ç»„æ•°: {len(duplicate_groups):,}")
    print(f"  é‡å¤æ–‡ä»¶æ•°: {total_duplicates:,}")
    if len(results) > 0:
        print(f"  å»é‡ç‡: {total_duplicates/len(results)*100:.2f}%")

    # æ‰“å°æ ‡ç­¾åˆ†å¸ƒç¤ºä¾‹
    print(f"\næ ‡ç­¾åˆ†å¸ƒç¤ºä¾‹ï¼ˆå‰10ä¸ªå”¯ä¸€ç»„åˆï¼‰:")
    for i, (content_hash, files) in enumerate(list(hash_groups.items())[:10], 1):
        labels = labels_info[content_hash]
        print(f"  ç»„åˆ{i}: æ ‡ç­¾={labels} æ–‡ä»¶æ•°={len(files)}")

    # ä»æ¯ç»„ä¸­é€‰æ‹©ä¸€ä¸ªæ–‡ä»¶ï¼ˆé€‰æ‹©æ–‡ä»¶åæœ€çŸ­çš„ï¼Œé€šå¸¸æ˜¯åŸå§‹æ–‡ä»¶ï¼‰
    unique_files = []
    for content_hash, files in hash_groups.items():
        # æŒ‰æ–‡ä»¶åé•¿åº¦æ’åºï¼Œé€‰æ‹©æœ€çŸ­çš„
        files.sort(key=lambda x: (len(x[0]), x[0]))
        unique_files.append(files[0][0])

    return unique_files, duplicate_groups, labels_info


def verify_stable_hash():
    """éªŒè¯ç¨³å®šå“ˆå¸Œçš„ä¸€è‡´æ€§"""
    import numpy as np

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = np.array([1.0, 2.0, 3.0, 4.0, 5.0], dtype=np.float32)

    # å¤šæ¬¡è®¡ç®—å“ˆå¸Œ
    hashes = []
    for _ in range(5):
        h = hashlib.md5(test_data.tobytes()).hexdigest()
        hashes.append(h)

    print("ç¨³å®šå“ˆå¸Œæµ‹è¯•:")
    print(f"æµ‹è¯•æ•°æ®: {test_data}")
    print(f"5æ¬¡MD5å“ˆå¸Œç»“æœ:")
    for i, h in enumerate(hashes):
        print(f"  {i+1}. {h}")

    all_same = len(set(hashes)) == 1
    print(f"æ‰€æœ‰å“ˆå¸Œç›¸åŒ: {'âœ…' if all_same else 'âŒ'}")

    return all_same


def main():
    parser = argparse.ArgumentParser(description='ECGæ•°æ®å»é‡ï¼ˆECG+æ ‡ç­¾ç»„åˆç‰ˆï¼‰')
    parser.add_argument('--data_dirs', nargs='+', required=True, help='æ•°æ®ç›®å½•åˆ—è¡¨')
    parser.add_argument('--output', type=str, default='unique_files_with_labels.json', help='è¾“å‡ºæ–‡ä»¶åˆ—è¡¨')
    parser.add_argument('--workers', type=int, default=32, help='å¹¶è¡Œè¿›ç¨‹æ•°')
    parser.add_argument('--save_duplicates', action='store_true', help='ä¿å­˜é‡å¤æ–‡ä»¶ä¿¡æ¯')
    parser.add_argument('--verify_hash', action='store_true', help='éªŒè¯å“ˆå¸Œç¨³å®šæ€§')
    parser.add_argument('--with_labels', action='store_true', help='ä½¿ç”¨ECG+æ ‡ç­¾ç»„åˆå»é‡')
    parser.add_argument('--num_classes', type=int, default=40, help='ç±»åˆ«æ•°')

    args = parser.parse_args()

    if args.verify_hash:
        verify_stable_hash()
        return

    print("=" * 70)
    if args.with_labels:
        print("ğŸš€ ECGæ•°æ®å»é‡ï¼ˆECG+æ ‡ç­¾ç»„åˆç‰ˆï¼‰")
        print("å…³é”®æ”¹è¿›ï¼šåŸºäºECGä¿¡å·+æ ‡ç­¾ç»„åˆè¿›è¡Œå»é‡")
        print("åªæœ‰ä¿¡å·å’Œæ ‡ç­¾éƒ½ç›¸åŒæ‰è®¤ä¸ºæ˜¯é‡å¤")
    else:
        print("ğŸš€ ECGæ•°æ®å»é‡ï¼ˆç¨³å®šå“ˆå¸Œç‰ˆï¼‰")
        print("å…³é”®æ”¹è¿›ï¼šä½¿ç”¨MD5æ›¿ä»£Pythonå†…ç½®hash()ï¼Œç¡®ä¿ä¸€è‡´æ€§")
    print("=" * 70)

    # æ”¶é›†æ–‡ä»¶
    def collect_files(data_dirs):
        all_files = []
        for data_dir in data_dirs:
            patterns = [
                os.path.join(data_dir, "*.txt"),
                os.path.join(data_dir, "**/*.txt"),
            ]
            for pattern in patterns:
                all_files.extend(glob.glob(pattern, recursive=True))
        return list(set(all_files))

    print("\næ”¶é›†æ–‡ä»¶...")
    all_files = collect_files(args.data_dirs)
    print(f"æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶")

    # å»é‡
    if args.with_labels:
        unique_files, duplicate_groups, labels_info = deduplicate_files_with_labels(
            all_files, args.num_classes, args.workers
        )
    else:
        unique_files, duplicate_groups = deduplicate_files_stable(all_files, args.workers)

    # ä¿å­˜å”¯ä¸€æ–‡ä»¶åˆ—è¡¨
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(unique_files, f, ensure_ascii=False, indent=2)

    print(f"\nå”¯ä¸€æ–‡ä»¶åˆ—è¡¨å·²ä¿å­˜: {args.output}")
    print(f"  åŒ…å« {len(unique_files):,} ä¸ªæ–‡ä»¶")

    # ä¿å­˜é‡å¤ä¿¡æ¯
    if args.save_duplicates and duplicate_groups:
        dup_output = args.output.replace('.json', '_duplicates.json')

        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        dup_info = []
        if args.with_labels:
            # ECG+æ ‡ç­¾ç‰ˆæœ¬
            for content_hash, files in list(duplicate_groups.items()):  # åªä¿å­˜å‰100ç»„
                labels = labels_info[content_hash]
                dup_info.append({
                    'hash': content_hash,
                    'labels': labels,
                    'count': len(files),
                    'files': [f[0] for f in files],
                    'sizes': [f[1] for f in files]
                })
        else:
            # ä»…ECGç‰ˆæœ¬
            for signal_hash, files in list(duplicate_groups.items()):  # åªä¿å­˜å‰100ç»„
                dup_info.append({
                    'hash': signal_hash,
                    'count': len(files),
                    'files': [f[0] for f in files],
                    'sizes': [f[1] for f in files]
                })

        with open(dup_output, 'w', encoding='utf-8') as f:
            json.dump(dup_info, f, ensure_ascii=False, indent=2)

        print(f"é‡å¤æ–‡ä»¶ä¿¡æ¯å·²ä¿å­˜: {dup_output}")

    # æ‰“å°é‡å¤ç¤ºä¾‹
    if duplicate_groups:
        print(f"\né‡å¤æ–‡ä»¶ç¤ºä¾‹ (å‰5ç»„):")
        if args.with_labels:
            # ECG+æ ‡ç­¾ç‰ˆæœ¬
            for i, (content_hash, files) in enumerate(list(duplicate_groups.items())[:5], 1):
                labels = labels_info[content_hash]
                print(f"\n  ç»„{i}: æ ‡ç­¾={labels} å“ˆå¸Œ={content_hash[:16]}... {len(files)} ä¸ªç›¸åŒæ–‡ä»¶")
                for f, size in files[:3]:
                    print(f"    - {f} ({size} bytes)")
                if len(files) > 3:
                    print(f"    ... è¿˜æœ‰ {len(files)-3} ä¸ªæ–‡ä»¶")
        else:
            # ä»…ECGç‰ˆæœ¬
            for i, (signal_hash, files) in enumerate(list(duplicate_groups.items())[:5], 1):
                print(f"\n  ç»„{i}: å“ˆå¸Œ={signal_hash[:16]}... {len(files)} ä¸ªç›¸åŒæ–‡ä»¶")
                for f, size in files[:3]:
                    print(f"    - {f} ({size} bytes)")
                if len(files) > 3:
                    print(f"    ... è¿˜æœ‰ {len(files)-3} ä¸ªæ–‡ä»¶")



if __name__ == "__main__":
    main()